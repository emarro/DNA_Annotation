# -*- coding: utf-8 -*-
"""Copy of inference_segment_nt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13v9IEcCnTN3H1Kc-tEm_AF2E8cOhWTFn

# Inference with Segment-NT models

## Installation and imports
"""

#!pip install biopython

import argparse
from Bio import SeqIO
import gzip
import numpy as np
import transformers
from transformers import AutoTokenizer, AutoModel
import torch
import seaborn as sns
import pandas as pd
from typing import List
import matplotlib.pyplot as plt

parser = argparse.ArgumentParser()
parser.add_argument("--fasta_path")
parser.add_argument("--reverse_complement", action="store_true")
args = parser.parse_args()

"""## Download the model
The following cell allows you to download the config and the model of one of the Segment-NT models.
"""


tokenizer = AutoTokenizer.from_pretrained("InstaDeepAI/segment_nt_multi_species", trust_remote_code=True)
model = AutoModel.from_pretrained("InstaDeepAI/segment_nt_multi_species", trust_remote_code=True, force_download=False)

"""# Define function that plots the probabilities"""

# seaborn settings
sns.set_style("whitegrid")
sns.set_context(
    "notebook",
    font_scale=1,
    rc={
        "font.size": 14,
        "axes.titlesize": 18,
        "axes.labelsize": 18,
        "xtick.labelsize": 16,
        "ytick.labelsize": 16,
        "legend.fontsize": 16,
        }
)

plt.rcParams['xtick.bottom'] = True
plt.rcParams['ytick.left'] = True

# set colors
colors = sns.color_palette("Set2").as_hex()
colors2 = sns.color_palette("husl").as_hex()


# Rearrange order of the features to match Fig.3 from the paper
features_rearranged = [
 'protein_coding_gene',
 'lncRNA',
 '5UTR',
 '3UTR',
 'exon',
 'intron',
 'splice_donor',
 'splice_acceptor',
 'promoter_Tissue_specific',
 'promoter_Tissue_invariant',
 'enhancer_Tissue_specific',
 'enhancer_Tissue_invariant',
 'CTCF-bound',
 'polyA_signal',
]

def plot_features(
    predicted_probabilities_all,
    seq_length: int,
    features: List[str],
    order_to_plot: List[str],
    fig_width=8,
):
    """
    Function to plot labels and predicted probabilities.

    Args:
        predicted_probabilities_all: Probabilities per genomic feature for each
            nucleotides in the DNA sequence.
        seq_length: DNA sequence length.
        feature: Genomic features to plot.
        order_to_plot: Order in which to plot the genomic features. This needs to be
            specified in order to match the order presented in the Fig.3 of the paper
        fig_width: Width of the figure
    """

    sc = 1.8
    n_panels = 7

    # fig, axes = plt.subplots(n_panels, 1, figsize=(fig_width * sc, (n_panels + 2) * sc), height_ratios=[6] + [2] * (n_panels-1))
    _, axes = plt.subplots(n_panels, 1, figsize=(fig_width * sc, (n_panels + 4) * sc))

    for n, feat in enumerate(order_to_plot):
        feat_id = features.index(feat)
        prob_dist = predicted_probabilities_all[:, feat_id]

        # Use the appropriate subplot
        ax = axes[n // 2]

        try:
            id_color = colors[feat_id]
        except:
            id_color = colors2[feat_id - 8]
        ax.plot(
            prob_dist,
            color=id_color,
            label=feat,
            linestyle="-",
            linewidth=1.5,
        )
        ax.set_xlim(0, seq_length)
        ax.grid(False)
        ax.spines['bottom'].set_color('black')
        ax.spines['top'].set_color('black')
        ax.spines['right'].set_color('black')
        ax.spines['left'].set_color('black')

    for a in range (0,n_panels):
        axes[a].set_ylim(0, 1.05)
        axes[a].set_ylabel("Prob.")
        axes[a].legend(loc="upper left", bbox_to_anchor=(1, 1), borderaxespad=0)
        if a != (n_panels-1):
            axes[a].tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=False)

    # Set common x-axis label
    axes[-1].set_xlabel("Nucleotides")
    # axes[0].axis('off')  # Turn off the axis
    axes[n_panels-1].grid(False)
    axes[n_panels-1].tick_params(axis='y', which='both', left=True, right=False, labelleft=True, labelright=False)

    axes[0].set_title("Probabilities predicted over all genomics features", fontweight="bold")

    plt.show()

"""# Get human chromosome 20

To reproduce the figures of the Segment-NT paper, we retrieve here the file of the
human chromosome 20
"""

#! wget https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.20.fa.gz

#fasta_path = "Homo_sapiens.GRCh38.dna.chromosome.20.fa.gz"
fasta_path = args.fasta_path
reverse_complement = args.reverse_complement
with gzip.open(fasta_path, "rt") as handle:
    record = next(SeqIO.parse(handle, "fasta"))
    if reverse_complement:
        print(f"Taking the reverse complement of {record.id}")
        chr20 = str(record.seq.complement())
    else:
      chr20 = str(record.seq)

len(chr20)

"""# Infer on 30kb genomic sequence (Does not require changing the rescaling factor in the forward function)

## Instantiate SegmentNT inference function
The following cell allows you to download the weights of one of the Segment-NT models. It returns the weights dictionary, the haiku forward function, the tokenizer and the config dictionary.

Just like for the `get_pretrained_nucleotide_transformer` function, you can also specify:
1. the layers at which you'd like to collect embeddings (e.g. (5, 10, 20) to get embeddings at layers 5, 10 and 20)
2. the attention maps youÂ´d like to collect (e.g. ((1,4), (7,18)) to get attention maps corresponding to layer 1 head number 4 and layer 7 head number 18). Please refer to the config to see the number of layers and heads in the model.
3. the maximum number of tokens in the sequences you'll compute the inference on. You can put values up to value specified in the model's config (counting the class token that will be added automatically at the beginning of the sequence), however we recommend keeping this number as small as possible for optimized memory and inference time.
"""

# The number of DNA tokens (excluding the CLS token prepended) needs to be dividible by
# the square of the number of downsampling block in SegmentNT UNet head, i.e 4.
# In the paper and in the jax colab, the length is set at 8333 tokens, which corresponds
# to 49992 nucleotides. On Google Colab, the inference with this length fits on the
# JAX model but does not fit in the Torch model. Therefore, we select here a slightly
# smaller length.
max_num_dna_tokens = 5000 #1668

# If max_num_tokens is larger than what was used to train Segment-NT, the rescaling
# factor needs to be adapted.
if max_num_dna_tokens + 1 > 5001:
    inference_rescaling_factor = (max_num_dna_tokens + 1) / 2048

    # Apply the new rescaling factor to all Rotary Embeddings layer.
    num_layers = len(model.esm.encoder.layer)

    for layer in range(num_layers):
      model.esm.encoder.layer[layer].attention.self.rotary_embeddings.rescaling_factor = inference_rescaling_factor
else:
    inference_rescaling_factor = None

"""## Tokenize the DNA sequence"""

idx_start = 2650520
idx_stop = idx_start + max_num_dna_tokens*6

sequences = [chr20[idx_start:idx_stop]]
tokens = tokenizer.batch_encode_plus(sequences, return_tensors="pt", padding="max_length", max_length = max_num_dna_tokens)["input_ids"]

"""## Infer on the resulting batch"""

idx_start = 2650520 #15502210
for _ in range(1): # Move window along
  idx_stop = idx_start + max_num_dna_tokens*6
  if reverse_complement:
    sequences = [chr20[idx_start:idx_stop][::-1]]
  else:
    sequences = [chr20[idx_start:idx_stop]]
  tokens = tokenizer.batch_encode_plus(sequences, return_tensors="pt", padding="max_length", max_length = max_num_dna_tokens)["input_ids"]
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  model.cuda()
  # Infer
  tokens = tokens.cuda()
  attention_mask = (tokens != tokenizer.pad_token_id).cuda()
  print(attention_mask.sum())
  with torch.no_grad():
    outs = model(
        tokens,
        attention_mask=attention_mask,
    )
  idx_start = idx_stop

# Obtain the logits over the genomic features
logits = outs["logits"]
# Transform them on probabilities
if reverse_complement:
    probabilities = np.asarray(torch.flip(torch.nn.functional.softmax(logits, dim=-1).cpu(), dims=[1]))[...,-1]
else:
    probabilities = np.asarray(torch.nn.functional.softmax(logits, dim=-1).cpu())[...,-1]
print(sequences[0])
print(tokens[0])
print(len(sequences[0]))
print(probabilities[0])
print(pd.Series(probabilities[0,:,model.config.features.index('splice_donor')]).describe())

tokenizer.get_vocab().keys()

"""## Notes about the outputs
1. The logits is of shape $\hat{y} \in \mathbb{R}^{\text{batch, num_bps, num_annotations, 2}}$.
2. The $\text{num_annotations} = 14$ used are:
 * 'protein_coding_gene',
 * 'lncRNA',
 * '5UTR',
 * '3UTR',
 * 'exon',
 * 'intron',
 * 'splice_donor',
 * 'splice_acceptor',
 * 'promoter_Tissue_specific',
 * 'promoter_Tissue_invariant',
 * 'enhancer_Tissue_specific',
 * 'enhancer_Tissue_invariant',
 * 'CTCF-bound',
 * 'polyA_signal'
3. The softmax they is over the last dimension of length 2 correspding to whether that given annotation is true/false. That is:
  * gene/not-gene
  * lncRNA/not-lncRNA
  * exon/not-exon
  * intron/not-intron
  * etc.
"""

batch, num_bps, num_annotations = probabilities.shape
print(probabilities[0])

torch.nn.functional.softmax(logits, dim=-1).shape

del outs

del tokens, attention_mask

"""## Plot the probabilities for 14 genomic features along this DNA sequence

Please note that Fig.1 from the SegmentNT paper is realized with SegmentNT-10kb, whereas here SegmentNT-30kb is used, which explains why the probabilities are not the exact same.
"""

#plot_features(
#    probabilities[0],
#    probabilities.shape[-2],
#    fig_width=20,
#    features=model.config.features,
#    order_to_plot=features_rearranged
#)

model.config.features

"""Annotate an entire chromosome.

"""

import gc
from tqdm import tqdm

agg = np.zeros((len(chr20), num_annotations))

"""Seek in the chromosome until we find a window that doesn't contain any `N` tokens."""

idx_start = 0 #26325000 #2650520
window_length = max_num_dna_tokens*6
while chr20[idx_start:idx_start+window_length].count('N') > 0: #0.5*window_length:
  idx_start += int(0.5*window_length)

"""
## Inference
Move in windows of 30kb, run a forward pass for each window, and save the output probs in `agg`. Segment NT is very finicky with the input length, so padded tokens (usually `N` tokens) cause a lot of problems, particularly at either end of the chromosome and around the centromere. We ignore these locations for now, we could reduced the context window size at around these points or do something more clever, but I don't have time for that right now and just ignoring them should give us enough to work off of in the short term."""

for idx_start in tqdm(range(idx_start, len(chr20), max_num_dna_tokens*6)): # Move window along
  gc.collect()
  torch.cuda.empty_cache()
  idx_stop = idx_start + max_num_dna_tokens*6
  if idx_stop > len(chr20):
    idx_stop = len(chr20) - 1
    idx_start = idx_stop - (max_num_dna_tokens * 6)
  if reverse_complement:
    sequences = [chr20[idx_start:idx_stop][::-1]]
  else:
    sequences = [chr20[idx_start:idx_stop]]
  #print(f"\nAnnotating sequence of length {len(sequences[0])} from {idx_start} - {idx_stop}")
  tokens = tokenizer.batch_encode_plus(sequences, return_tensors="pt", padding="max_length", max_length = max_num_dna_tokens)["input_ids"]
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  model.cuda()
  # Infer
  tokens = tokens.cuda()
  attention_mask = (tokens != tokenizer.pad_token_id).cuda()
  #print(attention_mask.sum())
  try:
    with torch.no_grad():
      outs = model(
          tokens,
          attention_mask=attention_mask,
      )
      logits = outs["logits"]
      # Transform them on probabilities
      if reverse_complement:
          probabilities = np.asarray(torch.flip(torch.nn.functional.softmax(logits, dim=-1).cpu(), dims=[1]))[...,-1]
      else:
          probabilities = np.asarray(torch.nn.functional.softmax(logits, dim=-1).cpu())[...,-1]
      #probabilities = np.asarray(torch.nn.functional.softmax(logits, dim=-1).cpu())[...,-1]
      agg[idx_start:idx_stop] = probabilities
  except Exception as e:
    print(f"\nError with sequence of length {len(sequences[0])}, unpadded tokens = {attention_mask.sum()} from {idx_start} - {idx_stop}")
    print("\nContinue")
  idx_start = idx_stop
if reverse_complement:
    np.save("agg_30kb_multi_species_rc_3to5.npy", agg)
else:
    np.save("agg_30kb_multi_species.npy", agg)
assert False

idx_stop

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# gzip agg_30kb_multi_species_reverse_complement.npy
# cp agg_30kb_multi_species.npy.gz /content/drive/MyDrive/

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cp agg_30kb_multi_species.npy.gz /content/drive/MyDrive/

"""# Infer and plot on 50kb genomic sequence (Requires changing the rescaling factor in the forward function)

## Instantiate SegmentNT inference function
"""

# The number of DNA tokens (excluding the CLS token prepended) needs to be dividible by
# the square of the number of downsampling block in SegmentNT UNet head, i.e 4.
# In the paper and in the jax colab, the length is set at 8333 tokens, which corresponds
# to 49992 nucleotides. On Google Colab, the inference with this length fits on the
# JAX model but does not fit in the Torch model. Therefore, we select here a slightly
# smaller length.
max_num_dna_tokens = 8200

# If max_num_tokens is larger than what was used to train Segment-NT, the rescaling
# factor needs to be adapted.
if max_num_dna_tokens + 1 > 5001:
    inference_rescaling_factor = (max_num_dna_tokens + 1) / 2048

    # Apply the new rescaling factor to all Rotary Embeddings layer.
    num_layers = len(model.esm.encoder.layer)

    for layer in range(num_layers):
      model.esm.encoder.layer[layer].attention.self.rotary_embeddings.rescaling_factor = inference_rescaling_factor
else:
    inference_rescaling_factor = None

"""## Tokenize the DNA sequence"""

idx_start = 5099984
idx_stop = idx_start + max_num_dna_tokens*6

sequences = [chr20[idx_start:idx_stop]]
tokens = tokenizer.batch_encode_plus(sequences, return_tensors="pt", padding="max_length", max_length = max_num_dna_tokens)["input_ids"]

"""## Infer on the resulting batch"""

import gc

gc.collect()
torch.cuda.empty_cache()

# Infer
tokens = tokens.cuda()
attention_mask = (tokens != tokenizer.pad_token_id).cuda()
with torch.no_grad():
  outs = model(
      tokens,
      attention_mask=attention_mask,
  )

# Obtain the logits over the genomic features
logits = outs["logits"]
# Transform them on probabilities
probabilities = np.asarray(torch.nn.functional.softmax(logits, dim=-1).cpu())[...,-1]

"""## Plot the probabilities for 14 genomic features along this DNA sequence"""

plot_features(
    probabilities[0],
    probabilities.shape[-2],
    fig_width=20,
    features=model.config.features,
    order_to_plot=features_rearranged
)
